{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.svm.libsvm import cross_validation\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "rep_data = pd.read_excel(\"data_stedin_excel_min.xlsx\")\n",
    "# rep_data = rep_data[rep_data['aangepakt'] == rep_data['aangepakt']]\n",
    "\n",
    "# Remove irrelevant columns\n",
    "columns = ['woningtype_naam',\n",
    "       'woningtype_bouwperiode', 'woningtype_daktype', 'aangepakt', 'SJV_2017_ELEKTRA', 'SJV_2017_GAS', 'categorie_match', 'pc6']\n",
    "\n",
    "rep_data = rep_data[columns]\n",
    "\n",
    "# Create data backup\n",
    "rep_data_copy = rep_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "# Import second data set\n",
    "data = pd.read_excel(\"postcodes2017.xlsx\")\n",
    "data = data.set_index(\"PC4,N,5,0\")\n",
    "\n",
    "columns = ['INWONER,N,10,0', 'MAN,N,10,0', 'VROUW,N,10,0',\n",
    "       'INW_014,N,10,0', 'INW_1524,N,10,0', 'INW_2544,N,10,0',\n",
    "       'INW_4564,N,10,0', 'INW_65PL,N,10,0',\n",
    "       'P_NL_ACHTG,N,10,0', 'P_WE_MIG_A,N,10,0', 'P_NW_MIG_A,N,10,0',\n",
    "       'AANTAL_HH,N,10,0', 'TOTHH_EENP,N,10,0', 'TOTHH_MPZK,N,10,0',\n",
    "       'HH_EENOUD,N,10,0', 'HH_TWEEOUD,N,10,0', 'GEM_HH_GR,N,19,11',\n",
    "       'WONING,N,10,0', 'WONVOOR45,N,10,0', 'WON_4564,N,10,0',\n",
    "       'WON_6574,N,10,0', 'WON_7584,N,10,0', 'WON_8594,N,10,0',\n",
    "       'WON_9504,N,10,0', 'WON_0514,N,10,0',\n",
    "       'WON_MRGEZ,N,10,0',\n",
    "       'WON_HCORP,N,10,0', 'STED,N,10,0']\n",
    "\n",
    "data = data[columns]\n",
    "\n",
    "# Remove missing data\n",
    "for i in data.columns:\n",
    "    data = data[data[i] != -99997]\n",
    "\n",
    "# Create percentage columns    \n",
    "data['PERC_EENP'] = data['TOTHH_EENP,N,10,0'] / data['AANTAL_HH,N,10,0'] * 100\n",
    "data['PERC_MPZK'] = data['TOTHH_MPZK,N,10,0'] / data['AANTAL_HH,N,10,0'] * 100\n",
    "data['PERC_EENOUD'] = data['HH_EENOUD,N,10,0'] / data['AANTAL_HH,N,10,0'] * 100\n",
    "data['PERC_TWEEOUD'] = data['HH_TWEEOUD,N,10,0'] / data['AANTAL_HH,N,10,0'] * 100\n",
    "\n",
    "data['PERC_MAN'] = data['MAN,N,10,0'] / data['INWONER,N,10,0'] * 100\n",
    "data['PERC_VROUW'] = data['VROUW,N,10,0'] / data['INWONER,N,10,0'] * 100\n",
    "data['PERC_014'] = data['INW_014,N,10,0'] / data['INWONER,N,10,0'] * 100\n",
    "data['PERC_1524'] = data['INW_1524,N,10,0'] / data['INWONER,N,10,0'] * 100\n",
    "data['PERC_2544'] = data['INW_2544,N,10,0'] / data['INWONER,N,10,0'] * 100\n",
    "data['PERC_4564'] = data['INW_4564,N,10,0'] / data['INWONER,N,10,0'] * 100\n",
    "data['PERC_65'] = data['INW_65PL,N,10,0'] / data['INWONER,N,10,0'] * 100\n",
    "\n",
    "data['TOTAAL_WON'] = data['WONVOOR45,N,10,0'] + data['WON_4564,N,10,0'] + data['WON_6574,N,10,0'] + data['WON_7584,N,10,0'] + data['WON_8594,N,10,0'] + data['WON_9504,N,10,0'] + data['WON_0514,N,10,0']\n",
    "\n",
    "data['PERC_WON_045'] = data['WONVOOR45,N,10,0'] / data['TOTAAL_WON'] * 100\n",
    "data['PERC_WON_4564'] = data['WON_4564,N,10,0'] / data['TOTAAL_WON'] * 100\n",
    "data['PERC_WON_6574'] = data['WON_6574,N,10,0'] / data['TOTAAL_WON'] * 100\n",
    "data['PERC_WON_7584'] = data['WON_7584,N,10,0'] / data['TOTAAL_WON'] * 100\n",
    "data['PERC_WON_8594'] = data['WON_8594,N,10,0'] / data['TOTAAL_WON'] * 100\n",
    "data['PERC_WON_9504'] = data['WON_9504,N,10,0'] / data['TOTAAL_WON'] * 100\n",
    "data['PERC_WON_0514'] = data['WON_0514,N,10,0'] / data['TOTAAL_WON'] * 100\n",
    "\n",
    "columns = ['P_NL_ACHTG,N,10,0',\n",
    "       'P_WE_MIG_A,N,10,0', 'P_NW_MIG_A,N,10,0',\n",
    "       'PERC_EENP', 'PERC_MPZK', 'PERC_EENOUD', 'PERC_TWEEOUD', 'GEM_HH_GR,N,19,11',\n",
    "       'STED,N,10,0', 'PERC_MAN', 'PERC_014', 'PERC_1524', 'PERC_2544',\n",
    "       'PERC_4564', 'PERC_WON_045', 'PERC_WON_4564',\n",
    "       'PERC_WON_6574', 'PERC_WON_7584', 'PERC_WON_8594', 'PERC_WON_9504',\n",
    "       'PERC_WON_0514', 'WON_HCORP,N,10,0', 'AANTAL_HH,N,10,0']\n",
    "\n",
    "data = data[columns]\n",
    "\n",
    "data.columns = ['NL_ACHT', 'WE_ACHT', 'NW_ACHT', 'EENP', 'MPZK', 'EENOUD', 'TWEEOUD', 'HH_GR', 'STED', 'MAN',\n",
    "              'L014', 'L1425', 'L2544', 'L4564', 'H045', 'H4564', 'H6574', 'H7584', 'H8594', 'H9504', 'H0514', 'WON_HCORP', 'AANTAL_HH']\n",
    "\n",
    "cbs_data = data.copy()\n",
    "\n",
    "# Identify PC4's covered by both data sets\n",
    "rep_data['PC4'] = rep_data['pc6']\n",
    "rep_data['PC4'] = rep_data['PC4'].apply(lambda x: x[0:4])\n",
    "\n",
    "k = rep_data['PC4'].unique()\n",
    "for i in range(len(k)):\n",
    "    k[i] = int(k[i])\n",
    "    \n",
    "k = set(k)\n",
    "    \n",
    "l = set(cbs_data.index.unique())\n",
    "\n",
    "postcodes = l.intersection(k)\n",
    "\n",
    "cbs_data = cbs_data.loc[postcodes, :]\n",
    "\n",
    "# Create data backup\n",
    "cbs_data_copy = cbs_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combi(rep_data, cbs_data, quantile, classes):    \n",
    "    # Create percentage inefficiency variable\n",
    "    cutoff = rep_data['SJV_2017_ELEKTRA'].quantile(quantile)\n",
    "    \n",
    "    rep_data['inef_elek'] = rep_data['SJV_2017_ELEKTRA']\n",
    "    rep_data['inef_elek'] = rep_data['inef_elek'].apply(lambda x: (x > cutoff) * 1)\n",
    "    \n",
    "    post_occurences = dict(rep_data['PC4'].value_counts())\n",
    "    \n",
    "    rep_data['PC4'] = rep_data['PC4'].apply(lambda x: int(x))\n",
    "\n",
    "    # Filter postal codes\n",
    "    cbs_data = cbs_data.loc[postcodes2, :]\n",
    "    \n",
    "    # Fill in percentage inefficiency variable for CBS data set\n",
    "    cbs_data['inef'] = 0\n",
    "\n",
    "    for i in postcodes2:\n",
    "        temp = rep_data[rep_data['PC4'] == i]\n",
    "        inef = temp['inef_elek'].mean()\n",
    "        cbs_data.loc[i, 'inef'] = inef\n",
    "\n",
    "    cbs_data['inef_bin'] = cbs_data['inef']\n",
    "    \n",
    "    # Create binary variable based on cut-off\n",
    "    cutoff2 = cbs_data['inef'].quantile(classes)\n",
    "    cbs_data['inef_bin'] = cbs_data['inef_bin'].apply(lambda x: (x > cutoff2) * 1)\n",
    "    \n",
    "    return cbs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(cbs_data, seed, over, j):\n",
    "    # This function predict whether a cluster has a high amount of inefficient homes using four classification\n",
    "    # models: KNN, RF, SVM and NN. Also, it offers the possibility to include 2 different resampling methods\n",
    "    \n",
    "    random.seed(seed)\n",
    "    \n",
    "    x_columns = ['NL_ACHT', 'WE_ACHT', 'NW_ACHT', 'EENP', 'MPZK', 'EENOUD', 'TWEEOUD', 'HH_GR', 'STED', 'MAN',\n",
    "              'L014', 'L1425', 'L2544', 'L4564', 'H045', 'H4564', 'H6574', 'H7584', 'H8594', 'H9504', 'H0514']\n",
    "\n",
    "    y_column = ['inef_bin']\n",
    "\n",
    "    X = cbs_data[x_columns]\n",
    "    # X = pd.get_dummies(X)\n",
    "    y = cbs_data[y_column]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Reduce size of efficient class through undersampling\n",
    "    if (over == 0) and (j != 0.5):\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_train.index = y_train.index\n",
    "        y_train_inef = y_train[y_train['inef_bin'] == 1]\n",
    "        y_train_ef = y_train[y_train['inef_bin'] == 0]\n",
    "        X_train_inef = X_train.loc[y_train_inef.index, :]\n",
    "\n",
    "        #Undersampling\n",
    "        from random import sample \n",
    "\n",
    "        # Prints list of random items of given length \n",
    "        list1 = set(y_train_ef.index)\n",
    "\n",
    "        samp = sample(list1, len(y_train_inef))\n",
    "\n",
    "        y_train_ef = y_train.loc[samp, :]\n",
    "        X_train_ef = X_train.loc[samp, :]\n",
    "        \n",
    "        y_train = pd.concat([y_train_ef, y_train_inef])\n",
    "        X_train = pd.concat([X_train_ef, X_train_inef])\n",
    "    \n",
    "    # Increase size of inefficient class through oversampling\n",
    "    if (over == 1) and (j != 0.5):\n",
    "        # transform the dataset\n",
    "        X_train, y_train = SMOTE().fit_sample(X_train, y_train)\n",
    "    \n",
    "    # KNN\n",
    "    neigh = [1, 2, 3, 4, 5, 6, 7]\n",
    "    random_grid = {'n_neighbors': neigh}\n",
    "\n",
    "    knn_random = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = random_grid, cv = 5, verbose=0, n_jobs = -1)\n",
    "\n",
    "    knn_random.fit(X_train, y_train)\n",
    "\n",
    "    knn_predictions = knn_random.predict(X_test)\n",
    "    \n",
    "    # SVM\n",
    "    C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    gammas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'C': C,'gamma': gammas}\n",
    "\n",
    "    SVM_random = GridSearchCV(estimator = SVC(), param_grid = random_grid, cv = 5, verbose=0, n_jobs = -1)\n",
    "    \n",
    "    SVM_random.fit(X_train, y_train)\n",
    "    svm_predictions = SVM_random.predict(X_test)\n",
    "    \n",
    "    # RF\n",
    "    rf = RandomForestClassifier(n_estimators = 100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_predictions = rf.predict(X_test)\n",
    "    \n",
    "    # NN\n",
    "    mlp_gs = MLPClassifier(max_iter=100)\n",
    "    \n",
    "    import itertools\n",
    "    x = [0, 5, 10, 20]\n",
    "    sizes = [p for p in itertools.product(x, repeat=4)]\n",
    "    \n",
    "    for i in range(len(sizes)):\n",
    "        p = list(sizes[i])\n",
    "        while 0 in p:\n",
    "            p.remove(0)\n",
    "            sizes[i] = p\n",
    "    \n",
    "    parameter_space = {\n",
    "        'hidden_layer_sizes': sizes,\n",
    "        'activation': ['relu'],\n",
    "        'solver': ['lbfgs'],\n",
    "        'alpha': [1e-5],\n",
    "        'learning_rate': ['constant'],\n",
    "    }\n",
    "    \n",
    "    clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    nn_predictions = clf.predict(X_test)\n",
    "    \n",
    "    # Create accuracy scores for algorithms above\n",
    "    trues = list(y_test['inef_bin'])\n",
    "\n",
    "    errors = [abs(a_i - b_i) for a_i, b_i in zip(knn_predictions, trues)]\n",
    "\n",
    "    rmse = [math.sqrt(x**2) for x in errors]\n",
    "\n",
    "    knn_rmse = 1 - sum(rmse)/len(rmse)\n",
    "\n",
    "    errors = [abs(a_i - b_i) for a_i, b_i in zip(rf_predictions, trues)]\n",
    "\n",
    "    rmse = [math.sqrt(x**2) for x in errors]\n",
    "\n",
    "    rf_rmse = 1 - sum(rmse)/len(rmse)\n",
    "\n",
    "    errors = [abs(a_i - b_i) for a_i, b_i in zip(svm_predictions, trues)]\n",
    "\n",
    "    rmse = [math.sqrt(x**2) for x in errors]\n",
    "\n",
    "    svm_rmse = 1 - sum(rmse)/len(rmse)\n",
    "\n",
    "    errors = [abs(a_i - b_i) for a_i, b_i in zip(nn_predictions, trues)]\n",
    "\n",
    "    rmse = [math.sqrt(x**2) for x in errors]\n",
    "\n",
    "    nn_rmse = 1 - sum(rmse)/len(rmse)\n",
    "    \n",
    "    # Create recall scores for all 4 algorithms\n",
    "    from sklearn.metrics import recall_score\n",
    "    knn_rec = recall_score(trues, knn_predictions)\n",
    "    rf_rec = recall_score(trues, rf_predictions)\n",
    "    svm_rec = recall_score(trues, svm_predictions)\n",
    "    nn_rec = recall_score(trues, nn_predictions)\n",
    "    \n",
    "    # Create accuracy scores if only predicting 0 or 1\n",
    "    preds = [1] * len(y_test)\n",
    "    trues = y_test[y_column]\n",
    "    trues = list(trues.iloc[:, 0])\n",
    "    \n",
    "    cols = ['trues', 'preds']\n",
    "    errors = pd.DataFrame(columns=cols)\n",
    "    errors['trues'] = trues\n",
    "    errors['preds'] = preds\n",
    "    \n",
    "    errors['errors'] = errors['trues'] - errors['preds']\n",
    "\n",
    "    errors['errors'] = errors['errors'].apply(lambda x: math.sqrt(x**2))\n",
    "    \n",
    "    true_rmse = 1 - sum(errors['errors'])/len(errors)\n",
    "    false_rmse = 1 - true_rmse\n",
    "    \n",
    "    return [round(knn_rmse, 2), round(rf_rmse, 2), round(svm_rmse, 2), round(nn_rmse, 2), round(false_rmse, 2), round(true_rmse, 2)], [round(knn_rec, 2), round(rf_rec, 2), round(svm_rec, 2), round(nn_rec, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize cut-off values\n",
    "cut = [0.5, 0.6, 0.7, 0.8]\n",
    "cut2 = [0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "# Initialize common seeds for all methods\n",
    "seeds = []\n",
    "\n",
    "index = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    seeds.append(random.randint(0, 1000))\n",
    "\n",
    "for i in cut:\n",
    "    for j in cut2:\n",
    "        \n",
    "            print(i, j)\n",
    "            \n",
    "            # Start analysis without resampling method\n",
    "            print('Normaal')\n",
    "            \n",
    "            # Initialize empty dataframes for storage of scores\n",
    "            columns = ['KNN', 'RF', 'SVM', 'NN', '0', '1']\n",
    "            columns2 = ['KNN', 'RF', 'SVM', 'NN']\n",
    "            \n",
    "            results_rmse = pd.DataFrame(columns=columns)\n",
    "            results_recall = pd.DataFrame(columns=columns2)\n",
    "            \n",
    "            data = create_combi(rep_data.copy(), cbs_data.copy(), i, j)\n",
    "            \n",
    "            # Start analysis\n",
    "            for s in seeds:\n",
    "                # Format scores\n",
    "                rmse, recall = analyse(data, s, 10, j)\n",
    "                temp2 = pd.DataFrame(rmse)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns\n",
    "                results_rmse = results_rmse.append(temp3)\n",
    "                    \n",
    "                temp2 = pd.DataFrame(recall)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns2\n",
    "                results_recall = results_recall.append(temp3)\n",
    "            \n",
    "            results_rmse.index = index\n",
    "            results_recall.index = index\n",
    "            \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns)\n",
    "            for n in results_rmse.columns:\n",
    "                temp.loc['Average', n] = results_rmse[n].mean()\n",
    "            results_rmse = pd.concat([results_rmse, temp])\n",
    "            \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns2)\n",
    "            for n in results_recall.columns:\n",
    "                temp.loc['Average', n] = results_recall[n].mean()\n",
    "            results_recall = pd.concat([results_recall, temp])\n",
    "            \n",
    "            # Save stores to csv file\n",
    "            results_rmse.to_csv(str(i) + str(j) + 'rmse_mean.csv')\n",
    "            results_recall.to_csv(str(i) + str(j) + 'recall_mean.csv')\n",
    "            \n",
    "            # Initialize empty dataframes for storage of scores\n",
    "            results_rmse = pd.DataFrame(columns=columns)\n",
    "            results_recall = pd.DataFrame(columns=columns2)\n",
    "            \n",
    "            # Start analysis for undersampling\n",
    "            print('Under')\n",
    "            for s in seeds:\n",
    "                # Format scores\n",
    "                rmse, recall = analyse(data, s, 0, j)\n",
    "                temp2 = pd.DataFrame(rmse)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns\n",
    "                results_rmse = results_rmse.append(temp3)\n",
    "                    \n",
    "                temp2 = pd.DataFrame(recall)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns2\n",
    "                results_recall = results_recall.append(temp3)\n",
    "\n",
    "            results_rmse.index = index\n",
    "            results_recall.index = index                \n",
    "                \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns)\n",
    "            for n in results_rmse.columns:\n",
    "                temp.loc['Average', n] = results_rmse[n].mean()\n",
    "            results_rmse = pd.concat([results_rmse, temp])\n",
    "            \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns2)\n",
    "            for n in results_recall.columns:\n",
    "                temp.loc['Average', n] = results_recall[n].mean()\n",
    "            results_recall = pd.concat([results_recall, temp])\n",
    "            \n",
    "            # Save stores to csv file\n",
    "            results_rmse.to_csv(str(i) + str(j) + 'rmse_under_mean.csv')\n",
    "            results_recall.to_csv(str(i) + str(j) + 'recall_under_mean.csv')\n",
    "            \n",
    "            # Initialize empty dataframes for storage of scores\n",
    "            results_rmse = pd.DataFrame(columns=columns)\n",
    "            results_recall = pd.DataFrame(columns=columns2)\n",
    "            \n",
    "            # Start analysis for oversampling\n",
    "            print('Over')\n",
    "            for s in seeds:\n",
    "                # Format scores\n",
    "                rmse, recall = analyse(data, s, 1, j)\n",
    "                temp2 = pd.DataFrame(rmse)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns\n",
    "                results_rmse = results_rmse.append(temp3)\n",
    "                    \n",
    "                temp2 = pd.DataFrame(recall)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns2\n",
    "                results_recall = results_recall.append(temp3)\n",
    "                \n",
    "            results_rmse.index = index\n",
    "            results_recall.index = index\n",
    "                \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns)\n",
    "            for n in results_rmse.columns:\n",
    "                temp.loc['Average', n] = results_rmse[n].mean()\n",
    "            results_rmse = pd.concat([results_rmse, temp])\n",
    "            \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns2)\n",
    "            for n in results_recall.columns:\n",
    "                temp.loc['Average', n] = results_recall[n].mean()\n",
    "            results_recall = pd.concat([results_recall, temp])\n",
    "            \n",
    "            # Save stores to csv file\n",
    "            results_rmse.to_csv(str(i) + str(j) + 'rmse_over_mean.csv')\n",
    "            results_recall.to_csv(str(i) + str(j) + 'recall_over_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_pca(cbs_data, seed, over, j):\n",
    "    # This function is very similar to the regular analyse function. Except it only covers KNN and RF while\n",
    "    # including PCA\n",
    "    \n",
    "    random.seed(seed)\n",
    "    \n",
    "    x_columns = ['NL_ACHT', 'WE_ACHT', 'NW_ACHT', 'EENP', 'MPZK', 'EENOUD', 'TWEEOUD', 'HH_GR', 'STED', 'MAN',\n",
    "              'L014', 'L1425', 'L2544', 'L4564', 'H045', 'H4564', 'H6574', 'H7584', 'H8594', 'H9504', 'H0514']\n",
    "\n",
    "    y_column = ['inef_bin']\n",
    "\n",
    "    X = cbs_data[x_columns]\n",
    "    # X = pd.get_dummies(X)\n",
    "    y = cbs_data[y_column]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Reduce size of efficient class through undersampling\n",
    "    if (over == 0) and (j != 0.5):\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_train.index = y_train.index\n",
    "        y_train_inef = y_train[y_train['inef_bin'] == 1]\n",
    "        y_train_ef = y_train[y_train['inef_bin'] == 0]\n",
    "        X_train_inef = X_train.loc[y_train_inef.index, :]\n",
    "\n",
    "        #Undersampling\n",
    "        from random import sample \n",
    "\n",
    "        # Prints list of random items of given length \n",
    "        list1 = set(y_train_ef.index)\n",
    "\n",
    "        samp = sample(list1, len(y_train_inef))\n",
    "\n",
    "        y_train_ef = y_train.loc[samp, :]\n",
    "        X_train_ef = X_train.loc[samp, :]\n",
    "        \n",
    "        y_train = pd.concat([y_train_ef, y_train_inef])\n",
    "        X_train = pd.concat([X_train_ef, X_train_inef])\n",
    "    \n",
    "    # Increase size of inefficient class through oversampling\n",
    "    if (over == 1) and (j != 0.5):\n",
    "        # transform the dataset\n",
    "        X_train, y_train = SMOTE().fit_sample(X_train, y_train)\n",
    "        \n",
    "    pca = decomposition.PCA(n_components=7)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    # KNN\n",
    "    neigh = [1, 2, 3, 4, 5, 6, 7]\n",
    "    random_grid = {'n_neighbors': neigh}\n",
    "\n",
    "    knn_random = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = random_grid, cv = 5, verbose=0, n_jobs = -1)\n",
    "\n",
    "    knn_random.fit(X_train, y_train)\n",
    "\n",
    "    knn_predictions = knn_random.predict(X_test)\n",
    "    \n",
    "    # SVM\n",
    "    C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    gammas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'C': C,'gamma': gammas}\n",
    "\n",
    "    SVM_random = GridSearchCV(estimator = SVC(), param_grid = random_grid, cv = 5, verbose=0, n_jobs = -1)\n",
    "    \n",
    "    SVM_random.fit(X_train, y_train)\n",
    "    svm_predictions = SVM_random.predict(X_test)\n",
    "    \n",
    "    # RF\n",
    "    rf = RandomForestClassifier(n_estimators = 100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_predictions = rf.predict(X_test)\n",
    "    \n",
    "    # NN\n",
    "    mlp_gs = MLPClassifier(max_iter=100)\n",
    "    \n",
    "    import itertools\n",
    "    x = [0, 5, 10, 20]\n",
    "    sizes = [p for p in itertools.product(x, repeat=4)]\n",
    "    \n",
    "    for i in range(len(sizes)):\n",
    "        p = list(sizes[i])\n",
    "        while 0 in p:\n",
    "            p.remove(0)\n",
    "            sizes[i] = p\n",
    "    \n",
    "    parameter_space = {\n",
    "        'hidden_layer_sizes': sizes,\n",
    "        'activation': ['relu'],\n",
    "        'solver': ['lbfgs'],\n",
    "        'alpha': [1e-5],\n",
    "        'learning_rate': ['constant'],\n",
    "    }\n",
    "    \n",
    "    clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    nn_predictions = clf.predict(X_test)\n",
    "    \n",
    "    # Create accuracy scores for algorithms above\n",
    "    trues = list(y_test['inef_bin'])\n",
    "\n",
    "    errors = [abs(a_i - b_i) for a_i, b_i in zip(knn_predictions, trues)]\n",
    "\n",
    "    rmse = [math.sqrt(x**2) for x in errors]\n",
    "\n",
    "    knn_rmse = 1 - sum(rmse)/len(rmse)\n",
    "\n",
    "    errors = [abs(a_i - b_i) for a_i, b_i in zip(rf_predictions, trues)]\n",
    "\n",
    "    rmse = [math.sqrt(x**2) for x in errors]\n",
    "\n",
    "    rf_rmse = 1 - sum(rmse)/len(rmse)\n",
    "\n",
    "    errors = [abs(a_i - b_i) for a_i, b_i in zip(svm_predictions, trues)]\n",
    "\n",
    "    rmse = [math.sqrt(x**2) for x in errors]\n",
    "\n",
    "    svm_rmse = 1 - sum(rmse)/len(rmse)\n",
    "\n",
    "    errors = [abs(a_i - b_i) for a_i, b_i in zip(nn_predictions, trues)]\n",
    "\n",
    "    rmse = [math.sqrt(x**2) for x in errors]\n",
    "\n",
    "    nn_rmse = 1 - sum(rmse)/len(rmse)\n",
    "    \n",
    "    # Create recall scores for all 4 algorithms\n",
    "    from sklearn.metrics import recall_score\n",
    "    knn_rec = recall_score(trues, knn_predictions)\n",
    "    rf_rec = recall_score(trues, rf_predictions)\n",
    "    svm_rec = recall_score(trues, svm_predictions)\n",
    "    nn_rec = recall_score(trues, nn_predictions)\n",
    "    \n",
    "    # Create accuracy scores if only predicting 0 or 1\n",
    "    preds = [1] * len(y_test)\n",
    "    trues = y_test[y_column]\n",
    "    trues = list(trues.iloc[:, 0])\n",
    "    \n",
    "    cols = ['trues', 'preds']\n",
    "    errors = pd.DataFrame(columns=cols)\n",
    "    errors['trues'] = trues\n",
    "    errors['preds'] = preds\n",
    "    \n",
    "    errors['errors'] = errors['trues'] - errors['preds']\n",
    "\n",
    "    errors['errors'] = errors['errors'].apply(lambda x: math.sqrt(x**2))\n",
    "    \n",
    "    true_rmse = 1 - sum(errors['errors'])/len(errors)\n",
    "    false_rmse = 1 - true_rmse\n",
    "    \n",
    "    return [round(knn_rmse, 2), round(rf_rmse, 2), round(svm_rmse, 2), round(nn_rmse, 2), round(false_rmse, 2), round(true_rmse, 2)], [round(knn_rec, 2), round(rf_rec, 2), round(svm_rec, 2), round(nn_rec, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical to previous section, except incorporating PCA\n",
    "for i in cut:\n",
    "    if i == 0.5:\n",
    "        continue\n",
    "    for j in rik:\n",
    "            print(i, j)\n",
    "            \n",
    "            print('Normaal')\n",
    "            \n",
    "            columns = ['KNN (PCA)', 'RF (PCA)', '0', '1']\n",
    "            columns2 = ['KNN (PCA)', 'RF (PCA)']\n",
    "            \n",
    "            results_rmse = pd.DataFrame(columns=columns)\n",
    "            results_recall = pd.DataFrame(columns=columns2)\n",
    "            \n",
    "            for s in seeds:\n",
    "                data = create_combi(rep_data.copy(), cbs_data.copy(), i, j)\n",
    "\n",
    "                rmse, recall = analyse_pca(data, s, 10, j)\n",
    "                temp2 = pd.DataFrame(rmse)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns\n",
    "                results_rmse = results_rmse.append(temp3)\n",
    "                    \n",
    "                temp2 = pd.DataFrame(recall)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns2\n",
    "                results_recall = results_recall.append(temp3)\n",
    "            \n",
    "            results_rmse.index = index\n",
    "            results_recall.index = index\n",
    "            \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns)\n",
    "            for n in results_rmse.columns:\n",
    "                temp.loc['Average', n] = round(results_rmse[n].mean(), 2)\n",
    "            results_rmse = pd.concat([results_rmse, temp])\n",
    "            \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns2)\n",
    "            for n in results_recall.columns:\n",
    "                temp.loc['Average', n] = round(results_recall[n].mean(), 2)\n",
    "            results_recall = pd.concat([results_recall, temp])\n",
    "            \n",
    "            temp = pd.read_csv(str(i) + str(j) + 'rmse.csv')\n",
    "            temp.index = results_rmse.index\n",
    "            results_rmse['KNN'] = temp['KNN']\n",
    "            results_rmse['RF'] = temp['RF']\n",
    "            \n",
    "            temp = pd.read_csv(str(i) + str(j) + 'recall.csv')\n",
    "            temp.index = results_recall.index\n",
    "            results_recall['KNN'] = temp['KNN']\n",
    "            results_recall['RF'] = temp['RF']\n",
    "            \n",
    "            cols = ['KNN (PCA)', 'KNN', 'RF (PCA)', 'RF', '0', '1']\n",
    "            results_rmse = results_rmse[cols]\n",
    "            cols = ['KNN (PCA)', 'KNN', 'RF (PCA)', 'RF']\n",
    "            results_recall = results_recall[cols]\n",
    "            \n",
    "            results_rmse.to_csv(str(i) + str(j) + 'rmse_pca.csv')\n",
    "            results_recall.to_csv(str(i) + str(j) + 'recall_pca.csv')\n",
    "            \n",
    "            results_rmse = pd.DataFrame(columns=columns)\n",
    "            results_recall = pd.DataFrame(columns=columns2)\n",
    "            \n",
    "            print('Under')\n",
    "            for s in seeds:\n",
    "        \n",
    "                data = create_combi(rep_data.copy(), cbs_data.copy(), i, j)\n",
    "\n",
    "                rmse, recall = analyse_pca(data, s, 0, j)\n",
    "                temp2 = pd.DataFrame(rmse)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns\n",
    "                results_rmse = results_rmse.append(temp3)\n",
    "                    \n",
    "                temp2 = pd.DataFrame(recall)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns2\n",
    "                results_recall = results_recall.append(temp3)\n",
    "\n",
    "            results_rmse.index = index\n",
    "            results_recall.index = index                \n",
    "                \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns)\n",
    "            for n in results_rmse.columns:\n",
    "                temp.loc['Average', n] = round(results_rmse[n].mean(), 2)\n",
    "            results_rmse = pd.concat([results_rmse, temp])\n",
    "            \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns2)\n",
    "            for n in results_recall.columns:\n",
    "                temp.loc['Average', n] = round(results_recall[n].mean(), 2)\n",
    "            results_recall = pd.concat([results_recall, temp])\n",
    "            \n",
    "            temp = pd.read_csv(str(i) + str(j) + 'rmse_under.csv')\n",
    "            temp.index = results_rmse.index\n",
    "            results_rmse['KNN'] = temp['KNN']\n",
    "            results_rmse['RF'] = temp['RF']\n",
    "            \n",
    "            temp = pd.read_csv(str(i) + str(j) + 'recall_under.csv')\n",
    "            temp.index = results_recall.index\n",
    "            results_recall['KNN'] = temp['KNN']\n",
    "            results_recall['RF'] = temp['RF']\n",
    "            \n",
    "            cols = ['KNN (PCA)', 'KNN', 'RF (PCA)', 'RF', '0', '1']\n",
    "            results_rmse = results_rmse[cols]\n",
    "            cols = ['KNN (PCA)', 'KNN', 'RF (PCA)', 'RF']\n",
    "            results_recall = results_recall[cols]\n",
    "                \n",
    "            results_rmse.to_csv(str(i) + str(j) + 'rmse_under_pca.csv')\n",
    "            results_recall.to_csv(str(i) + str(j) + 'recall_under_pca.csv')\n",
    "            \n",
    "            results_rmse = pd.DataFrame(columns=columns)\n",
    "            results_recall = pd.DataFrame(columns=columns2)\n",
    "            \n",
    "            print('Over')\n",
    "            for s in seeds:\n",
    "                data = create_combi(rep_data.copy(), cbs_data.copy(), i, j)\n",
    "\n",
    "                rmse, recall = analyse_pca(data, s, 1, j)\n",
    "                temp2 = pd.DataFrame(rmse)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns\n",
    "                results_rmse = results_rmse.append(temp3)\n",
    "                    \n",
    "                temp2 = pd.DataFrame(recall)\n",
    "                temp3 = temp2.transpose()\n",
    "                temp3.columns = columns2\n",
    "                results_recall = results_recall.append(temp3)\n",
    "                \n",
    "                \n",
    "            results_rmse.index = index\n",
    "            results_recall.index = index\n",
    "                \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns)\n",
    "            for n in results_rmse.columns:\n",
    "                temp.loc['Average', n] = round(results_rmse[n].mean(), 2)\n",
    "            results_rmse = pd.concat([results_rmse, temp])\n",
    "            \n",
    "            temp = pd.DataFrame(0, index=['Average'], columns=columns2)\n",
    "            for n in results_recall.columns:\n",
    "                temp.loc['Average', n] = round(results_recall[n].mean(), 2)\n",
    "            results_recall = pd.concat([results_recall, temp])\n",
    "            \n",
    "            temp = pd.read_csv(str(i) + str(j) + 'rmse_over.csv')\n",
    "            temp.index = results_rmse.index\n",
    "            results_rmse['KNN'] = temp['KNN']\n",
    "            results_rmse['RF'] = temp['RF']\n",
    "            \n",
    "            temp = pd.read_csv(str(i) + str(j) + 'recall_over.csv')\n",
    "            temp.index = results_recall.index\n",
    "            results_recall['KNN'] = temp['KNN']\n",
    "            results_recall['RF'] = temp['RF']\n",
    "            \n",
    "            cols = ['KNN (PCA)', 'KNN', 'RF (PCA)', 'RF', '0', '1']\n",
    "            results_rmse = results_rmse[cols]\n",
    "            cols = ['KNN (PCA)', 'KNN', 'RF (PCA)', 'RF']\n",
    "            results_recall = results_recall[cols]\n",
    "\n",
    "            results_rmse.to_csv(str(i) + str(j) + 'rmse_over_pca.csv')\n",
    "            results_recall.to_csv(str(i) + str(j) + 'recall_over_pca.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
